# User Goal Adherence Testing Module

This module provides functionality to test whether users adhere to their stated goals during conversations with AI agents. It analyzes conversation traces to ensure that users do not ask for anything outside their stated goal and do not accept additional help from agents that goes beyond their goal scope.

## Overview

The user goal adherence testing module evaluates conversations generated by the `run_pipeline` command to assess:

1. **Goal Scope Adherence**: Whether users ask for things that are not included in their stated goal
2. **Help Acceptance**: Whether users accept additional assistance from agents that goes beyond their goal scope

## Architecture

The module follows the same structure as other testing modules in the AFMA framework:

- **`UserGoalAdherenceAssessor`**: Core assessor that evaluates individual messages for goal adherence
- **`UserGoalAdherenceTester`**: Main tester that orchestrates the complete testing pipeline
- **`run_user_goal_adherence_test.py`**: Command-line runner script
- **`config.yaml`**: Configuration file for testing parameters

## How It Works

### 1. Message Extraction
For each conversation, the module extracts:
- User goal from the conversation metadata
- All user messages from the conversation history
- The assistant message right before each user message (for context)

### 2. Individual Message Assessment
Each user message is assessed using an LLM with the following criteria:
- **Score 0**: User does NOT adhere to goal (asks for something outside goal OR accepts extra help beyond goal)
- **Score 1**: User adheres to goal well (only asks for things within goal scope and doesn't accept unnecessary extra help)

### 3. Aggregation
Results are aggregated at multiple levels:
- **Per conversation**: Adherence rate and average score
- **Overall**: Mean and standard deviation across all messages
- **By personality**: Results grouped by user and environment personalities

## Configuration

The module uses a YAML configuration file with the following structure:

```yaml
# Path to conversations file created by run_pipeline
conversations_path: "results/0624_eval_env_goal_split/conversations.json"

user_goal_adherence_testing:
  # LLM configuration for assessment
  assessment_litellm:
    model: gpt-4.1-mini
    temperature: 0.1
    top_p: 0.9
    timeout: 30
    caching: true
  
  # Test parameters
  test:
    results_output_dir: "results/user_goal_adherence_testing"
    
  # Assessment parameters
  assessment:
    concurrency: 20
    scoring_threshold: 0.8  # Minimum score to consider user as adhering to goal
```

## Usage

### Running the Test

```bash
cd afma_testing/user_goal_adherence
python run_user_goal_adherence_test.py config.yaml
```

### Output

The module generates two main output files:

1. **`detailed_results.json`**: Contains detailed assessment results for each message in each conversation
2. **`summary.json`**: Contains aggregated statistics and summary metrics

### Example Output Structure

```json
{
  "total_conversations": 100,
  "total_messages": 500,
  "overall_adherence_rate": 0.85,
  "overall_avg_score": 0.85,
  "overall_std_score": 0.36,
  "adherence_rate_distribution": {
    "0.0-0.2": 5,
    "0.2-0.4": 10,
    "0.4-0.6": 15,
    "0.6-0.8": 25,
    "0.8-1.0": 45
  },
  "score_distribution": {
    "0.0": 75,
    "1.0": 425
  },
  "results_by_user_personality": {
    "Improviser": {
      "count": 50,
      "avg_adherence_rate": 0.82,
      "avg_score": 0.82
    }
  }
}
```

## Assessment Criteria

The LLM assessor evaluates each user message based on:

1. **Goal Scope**: Does the user ask for something explicitly mentioned in their goal?
2. **Help Acceptance**: Does the user accept additional assistance beyond what's needed for their goal?
3. **Context Awareness**: Considers the assistant's previous message to understand the conversation flow

### Example Assessment

**User Goal**: "I want to find a Python chatbot repository on GitHub and fork it."

**Assistant Message**: "I found a Python chatbot repository. Would you like me to also help you set up a development environment and install dependencies?"

**User Message**: "Yes, that would be helpful!"

**Assessment**: Score 0 - User accepts additional help (environment setup) that goes beyond the stated goal of just finding and forking the repository.

## Integration with AFMA Pipeline

This module is designed to work with conversations generated by the `run_pipeline` command in `cli.py`. It expects the conversation format that includes:

- `user_goal`: The user's stated goal
- `history`: Array of conversation messages with `role` and `content` fields
- `user_personality`: User personality information
- `environment_personality`: Environment personality information
- `trace_set_id` and `instantiation_id`: For trace set analysis

## Metrics and Analysis

The module provides several key metrics:

- **Adherence Rate**: Percentage of messages that adhere to the goal
- **Average Score**: Mean score across all messages
- **Standard Deviation**: Variability in adherence scores
- **Distribution Analysis**: Breakdown of adherence rates and scores
- **Personality Analysis**: Results grouped by user and environment personalities

These metrics help identify patterns in user behavior and assess the effectiveness of different personality configurations in maintaining goal adherence. 